{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2383170,"sourceType":"datasetVersion","datasetId":1440465}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Imports\n","metadata":{}},{"cell_type":"code","source":"import torch   # main library\nimport torch.nn as nn # Used for getting the NN Layers\nfrom torch.optim import Adam  # Adam optimizer\nfrom torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creatning the objects\nfrom torchsummary import summary # Visualize the model layers and number of parameters\n\n#sklearn\nfrom sklearn.model_selection import train_test_split # Split the dataset (train, validation, test)\nfrom sklearn.metrics import accuracy_score # Calculate the testing Accuracy\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt # Plotting the training progress at the end\nimport pandas as pd # Data reading and preprocessing\nimport numpy as np # Mathematical operations\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu' # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.127300Z","iopub.execute_input":"2025-09-25T09:24:29.127568Z","iopub.status.idle":"2025-09-25T09:24:29.133122Z","shell.execute_reply.started":"2025-09-25T09:24:29.127548Z","shell.execute_reply":"2025-09-25T09:24:29.132415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Dataset","metadata":{}},{"cell_type":"code","source":" df=pd.read_csv(r'/kaggle/input/rice-type-classification/riceClassification.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.142928Z","iopub.execute_input":"2025-09-25T09:24:29.143137Z","iopub.status.idle":"2025-09-25T09:24:29.188334Z","shell.execute_reply.started":"2025-09-25T09:24:29.143122Z","shell.execute_reply":"2025-09-25T09:24:29.187754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dropna(inplace = True) # Drop missing/null values\ndf.drop([\"id\"], axis =1, inplace = True) # Drop Id column\nprint(\"Output possibilities: \", df[\"Class\"].unique()) # Possible Outputs\nprint(\"Data Shape (rows, cols): \", df.shape) # Print data shape\ndf.head() # Print/visualize the first 5 rows of the data\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.189332Z","iopub.execute_input":"2025-09-25T09:24:29.189523Z","iopub.status.idle":"2025-09-25T09:24:29.203815Z","shell.execute_reply.started":"2025-09-25T09:24:29.189509Z","shell.execute_reply":"2025-09-25T09:24:29.203328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Class'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.204563Z","iopub.execute_input":"2025-09-25T09:24:29.204776Z","iopub.status.idle":"2025-09-25T09:24:29.210063Z","shell.execute_reply.started":"2025-09-25T09:24:29.204761Z","shell.execute_reply":"2025-09-25T09:24:29.209401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_df = df.copy() # Creating a copy of the original Dataframe to use to normalize inference\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.211735Z","iopub.execute_input":"2025-09-25T09:24:29.212308Z","iopub.status.idle":"2025-09-25T09:24:29.225724Z","shell.execute_reply.started":"2025-09-25T09:24:29.212287Z","shell.execute_reply":"2025-09-25T09:24:29.225213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Splitting\n\n`Training Size 70%\nValidation Size 15%\nTesting Size 15% `","metadata":{}},{"cell_type":"code","source":"X=np.array(df.drop('Class',axis=1))\ny=np.array(df['Class'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state=42) # Create the training split\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.5,random_state=42) # Create the validation split\n\nprint(\"Training set is: \", X_train.shape[0], \" rows which is \", round(X_train.shape[0]/df.shape[0],4)*100, \"%\") # Print training shape\nprint(\"Validation set is: \",X_val.shape[0], \" rows which is \", round(X_val.shape[0]/df.shape[0],4)*100, \"%\") # Print validation shape\nprint(\"Testing set is: \",X_test.shape[0], \" rows which is \", round(X_test.shape[0]/df.shape[0],4)*100, \"%\") # Print testing shape\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.226444Z","iopub.execute_input":"2025-09-25T09:24:29.226685Z","iopub.status.idle":"2025-09-25T09:24:29.247704Z","shell.execute_reply.started":"2025-09-25T09:24:29.226662Z","shell.execute_reply":"2025-09-25T09:24:29.247158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Normalization","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\n\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.248359Z","iopub.execute_input":"2025-09-25T09:24:29.248587Z","iopub.status.idle":"2025-09-25T09:24:29.260636Z","shell.execute_reply.started":"2025-09-25T09:24:29.248566Z","shell.execute_reply":"2025-09-25T09:24:29.260134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Dataset Object","metadata":{}},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self,X,y):\n       self.X = torch.tensor(X, dtype=torch.float32).to(device)\n       self.y = torch.tensor(y, dtype=torch.float32).to(device)\n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        return self.X[index], self.y[index]\n\ntraining_dataset = dataset(X_train,y_train)\nvalidation_dataset = dataset(X_val, y_val)\ntesting_dataset = dataset(X_test, y_test)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.262156Z","iopub.execute_input":"2025-09-25T09:24:29.262788Z","iopub.status.idle":"2025-09-25T09:24:29.274201Z","shell.execute_reply.started":"2025-09-25T09:24:29.262771Z","shell.execute_reply":"2025-09-25T09:24:29.273681Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Hyper Parameters","metadata":{}},{"cell_type":"code","source":"EPOCHS=10\nBATCH_SIZE=32\nHIDDEN_NEURONS = 15\n\nLR = 1e-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.274910Z","iopub.execute_input":"2025-09-25T09:24:29.275434Z","iopub.status.idle":"2025-09-25T09:24:29.289128Z","shell.execute_reply.started":"2025-09-25T09:24:29.275413Z","shell.execute_reply":"2025-09-25T09:24:29.288588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Data Loaders","metadata":{}},{"cell_type":"code","source":"train_dataloader= DataLoader(dataset=training_dataset, batch_size=BATCH_SIZE,shuffle=True)\nvalid_dataloader= DataLoader(dataset=validation_dataset, batch_size=BATCH_SIZE,shuffle=True)\ntest_dataloader= DataLoader(dataset=testing_dataset, batch_size=BATCH_SIZE,shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.289674Z","iopub.execute_input":"2025-09-25T09:24:29.289863Z","iopub.status.idle":"2025-09-25T09:24:29.306474Z","shell.execute_reply.started":"2025-09-25T09:24:29.289829Z","shell.execute_reply":"2025-09-25T09:24:29.305978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Bulid Model","metadata":{}},{"cell_type":"code","source":"class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel,self).__init__()\n        self.input_layer=nn.Linear(X.shape[1],HIDDEN_NEURONS)\n        self.fc1=nn.Linear(HIDDEN_NEURONS,1)\n        self.sigmoid=nn.Sigmoid()\n        \n    def forward(self,x):\n        x=self.input_layer(x)\n        x=self.fc1(x)\n        x=self.sigmoid(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.306993Z","iopub.execute_input":"2025-09-25T09:24:29.307159Z","iopub.status.idle":"2025-09-25T09:24:29.330858Z","shell.execute_reply.started":"2025-09-25T09:24:29.307146Z","shell.execute_reply":"2025-09-25T09:24:29.330352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Creat Model ","metadata":{}},{"cell_type":"code","source":"model = MyModel().to(device)\nsummary(model, (X.shape[1],))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.331453Z","iopub.execute_input":"2025-09-25T09:24:29.331701Z","iopub.status.idle":"2025-09-25T09:24:29.347993Z","shell.execute_reply.started":"2025-09-25T09:24:29.331684Z","shell.execute_reply":"2025-09-25T09:24:29.347423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"optimizer= Adam(model.parameters(),lr=LR)\ncriterion= nn.BCELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.348693Z","iopub.execute_input":"2025-09-25T09:24:29.348898Z","iopub.status.idle":"2025-09-25T09:24:29.361023Z","shell.execute_reply.started":"2025-09-25T09:24:29.348883Z","shell.execute_reply":"2025-09-25T09:24:29.360517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11. Training ","metadata":{}},{"cell_type":"code","source":"total_loss_train_plot = []\ntotal_loss_validation_plot = []\ntotal_acc_train_plot = []\ntotal_acc_validation_plot = []\n\nfor epoch in range(EPOCHS):\n    total_acc_train = 0\n    total_loss_train = 0\n    total_acc_val = 0\n    total_loss_val = 0\n    ## Training and Validation\n    for data in train_dataloader:\n\n        inputs, labels = data\n\n        prediction = model(inputs).squeeze(1)\n\n        batch_loss = criterion(prediction, labels)\n\n        total_loss_train += batch_loss.item()\n\n        acc = ((prediction).round() == labels).sum().item()\n\n        total_acc_train += acc\n\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n\n    ## Validation\n    with torch.no_grad():\n        for data in valid_dataloader:\n            inputs, labels = data\n\n            prediction = model(inputs).squeeze(1)\n\n            batch_loss = criterion(prediction, labels)\n\n            total_loss_val += batch_loss.item()\n\n            acc = ((prediction).round() == labels).sum().item()\n\n            total_acc_val += acc\n\n    total_loss_train_plot.append(round(total_loss_train/1000, 4))\n    total_loss_validation_plot.append(round(total_loss_val/1000, 4))\n    total_acc_train_plot.append(round(total_acc_train/(training_dataset.__len__())*100, 4))\n    total_acc_validation_plot.append(round(total_acc_val/(validation_dataset.__len__())*100, 4))\n\n    print(f'''Epoch no. {epoch + 1} Train Loss: {total_loss_train/1000:.4f} Train Accuracy: {(total_acc_train/(training_dataset.__len__())*100):.4f} Validation Loss: {total_loss_val/1000:.4f} Validation Accuracy: {(total_acc_val/(validation_dataset.__len__())*100):.4f}''')\n    print(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:29.362519Z","iopub.execute_input":"2025-09-25T09:24:29.362702Z","iopub.status.idle":"2025-09-25T09:24:35.809544Z","shell.execute_reply.started":"2025-09-25T09:24:29.362689Z","shell.execute_reply":"2025-09-25T09:24:35.808782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12. Testing","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n  total_loss_test = 0\n  total_acc_test = 0\n  for data in test_dataloader:\n    inputs, labels = data\n\n    prediction = model(inputs).squeeze(1)\n\n    batch_loss_test = criterion((prediction), labels)\n    total_loss_test += batch_loss_test.item()\n    acc = ((prediction).round() == labels).sum().item()\n    total_acc_test += acc\n\nprint(f\"Accuracy Score is: {round((total_acc_test/X_test.shape[0])*100, 2)}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:35.810403Z","iopub.execute_input":"2025-09-25T09:24:35.810673Z","iopub.status.idle":"2025-09-25T09:24:35.872032Z","shell.execute_reply.started":"2025-09-25T09:24:35.810647Z","shell.execute_reply":"2025-09-25T09:24:35.871498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 13. Plotting and Visualizations\n","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\naxs[0].plot(total_loss_train_plot, label='Training Loss')\naxs[0].plot(total_loss_validation_plot, label='Validation Loss')\naxs[0].set_title('Training and Validation Loss over Epochs')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].set_ylim([0, 2])\naxs[0].legend()\n\naxs[1].plot(total_acc_train_plot, label='Training Accuracy')\naxs[1].plot(total_acc_validation_plot, label='Validation Accuracy')\naxs[1].set_title('Training and Validation Accuracy over Epochs')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Accuracy')\naxs[1].set_ylim([0, 100])\naxs[1].legend()\n\nplt.tight_layout()\n\nplt.show()\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:35.872641Z","iopub.execute_input":"2025-09-25T09:24:35.872872Z","iopub.status.idle":"2025-09-25T09:24:36.254090Z","shell.execute_reply.started":"2025-09-25T09:24:35.872834Z","shell.execute_reply":"2025-09-25T09:24:36.253416Z"}},"outputs":[],"execution_count":null}]}